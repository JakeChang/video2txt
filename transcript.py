"""
Whisper èªéŸ³è­˜åˆ¥æ¨¡çµ„
ä½¿ç”¨ OpenAI Whisper å°‡éŸ³æª”è½‰æ›ç‚ºæ–‡å­—
"""

import os
import whisper
from datetime import datetime
from pathlib import Path


# å–å¾—å°ˆæ¡ˆæ ¹ç›®éŒ„
BASE_DIR = Path(__file__).parent


def transcript_to_text(audio_path: str, video_id: str) -> str:
    """
    å°‡éŸ³æª”è½‰æ›ç‚ºæ–‡ç¨¿ (ä½¿ç”¨ medium æ¨¡å‹)

    Args:
        audio_path: éŸ³æª”è·¯å¾‘
        video_id: å½±ç‰‡è­˜åˆ¥ç¢¼

    Returns:
        æ–‡ç¨¿æª”æ¡ˆè·¯å¾‘
    """
    try:
        print(f"ğŸ™ï¸ é–‹å§‹ä½¿ç”¨æœ¬åœ° Whisper è½‰æ›éŸ³æª”: {audio_path}")

        # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
        if not os.path.exists(audio_path):
            raise FileNotFoundError("éŸ³æª”æª”æ¡ˆä¸å­˜åœ¨")

        # æª¢æŸ¥æª”æ¡ˆå¤§å°
        file_size_mb = os.path.getsize(audio_path) / (1024 * 1024)
        print(f"ğŸ“ éŸ³æª”å¤§å°: {file_size_mb:.2f} MB")

        if file_size_mb > 100:
            print("âš ï¸ è­¦å‘Š: éŸ³æª”æª”æ¡ˆè¼ƒå¤§ï¼Œè½‰æ›å¯èƒ½éœ€è¦è¼ƒé•·æ™‚é–“")

        # è¼‰å…¥ Whisper æ¨¡å‹
        print("ğŸ”„ æ­£åœ¨è¼‰å…¥ Whisper medium æ¨¡å‹...")
        model = whisper.load_model("medium")

        # åŸ·è¡Œè½‰æ›
        print("ğŸ”„ æ­£åœ¨ä½¿ç”¨ Whisper æ¨¡å‹è½‰æ›éŸ³æª”...")
        result = model.transcribe(
            audio_path,
            language=None,  # è‡ªå‹•åµæ¸¬èªè¨€
            task="transcribe",  # ä¸ç¿»è­¯ï¼Œä¿æŒåŸèªè¨€
            verbose=True
        )

        print("âœ… Whisper è½‰æ›æˆåŠŸ")

        # å„²å­˜æ–‡ç¨¿åˆ°æª”æ¡ˆ
        output_dir = BASE_DIR / "data"
        output_dir.mkdir(exist_ok=True)
        transcript_path = output_dir / f"{video_id}.raw.txt"

        # æ·»åŠ æ–‡ä»¶æ¨™é ­å’Œæ™‚é–“æˆ³
        timestamp = datetime.now().strftime("%Y/%m/%d %H:%M:%S")
        content = f"""# {video_id} æ–‡ç¨¿
è½‰æ›æ™‚é–“: {timestamp}
è½‰æ›å·¥å…·: openai-whisper (æœ¬åœ°æ¨¡å‹)
éŸ³æª”å¤§å°: {file_size_mb:.2f} MB

---

{result['text']}
"""

        # ç¢ºä¿å…§å®¹ä¸ç‚ºç©º
        if not result['text'].strip():
            raise ValueError("Whisper ç„¡æ³•å¾éŸ³æª”ä¸­æå–æ–‡å­—å…§å®¹")

        transcript_path.write_text(content, encoding="utf-8")
        print(f"ğŸ“ æ–‡ç¨¿å·²å„²å­˜è‡³: {transcript_path}")

        # ç”Ÿæˆ SRT å­—å¹•æª”æ¡ˆ
        srt_path = output_dir / f"{video_id}.srt"
        srt_content = generate_srt_from_segments(result.get('segments', []))
        if srt_content:
            srt_path.write_text(srt_content, encoding="utf-8")
            print(f"ğŸ“º SRT å­—å¹•å·²å„²å­˜è‡³: {srt_path}")

        # æ¸…ç†éŸ³æª”
        _cleanup_audio_files(audio_path)

        return str(transcript_path)

    except Exception as error:
        print(f"âŒ Whisper è½‰æ›æ–‡ç¨¿éŒ¯èª¤: {error}")

        # å»ºç«‹éŒ¯èª¤æ–‡ç¨¿æª”æ¡ˆ
        placeholder_content = f"# {video_id} æ–‡ç¨¿\n\n"
        placeholder_content += f"[Whisper è½‰æ›å¤±æ•—: {error}]\n\n"
        placeholder_content += f"éŸ³æª”è·¯å¾‘: {audio_path}\n"

        output_dir = BASE_DIR / "data"
        output_dir.mkdir(exist_ok=True)
        transcript_path = output_dir / f"{video_id}.raw.txt"
        transcript_path.write_text(placeholder_content, encoding="utf-8")

        # ä»ç„¶å˜—è©¦æ¸…ç†éŸ³æª”
        _cleanup_audio_files(audio_path)

        return str(transcript_path)


def transcript_to_text_fast(audio_path: str, video_id: str) -> str:
    """
    ä½¿ç”¨æ›´å°çš„æ¨¡å‹ä»¥æé«˜é€Ÿåº¦ (tiny æ¨¡å‹)

    Args:
        audio_path: éŸ³æª”è·¯å¾‘
        video_id: å½±ç‰‡è­˜åˆ¥ç¢¼

    Returns:
        æ–‡ç¨¿æª”æ¡ˆè·¯å¾‘
    """
    try:
        print("ğŸš€ ä½¿ç”¨å¿«é€Ÿæ¨¡å¼è½‰æ›éŸ³æª”...")

        # è¼‰å…¥ tiny æ¨¡å‹
        print("ğŸ”„ æ­£åœ¨è¼‰å…¥ Whisper tiny æ¨¡å‹...")
        model = whisper.load_model("tiny")

        # åŸ·è¡Œè½‰æ›
        result = model.transcribe(
            audio_path,
            language=None,
            task="transcribe",
            verbose=True
        )

        # å„²å­˜æ–‡ç¨¿
        output_dir = BASE_DIR / "data"
        output_dir.mkdir(exist_ok=True)
        transcript_path = output_dir / f"{video_id}.raw.txt"

        timestamp = datetime.now().strftime("%Y/%m/%d %H:%M:%S")
        content = f"""# {video_id} æ–‡ç¨¿ (å¿«é€Ÿæ¨¡å¼)
è½‰æ›æ™‚é–“: {timestamp}
è½‰æ›å·¥å…·: openai-whisper Tiny Model (æœ¬åœ°)

---

{result['text']}
"""

        transcript_path.write_text(content, encoding="utf-8")
        print(f"ğŸ“ å¿«é€Ÿæ¨¡å¼æ–‡ç¨¿å·²å„²å­˜è‡³: {transcript_path}")

        # ç”Ÿæˆ SRT å­—å¹•æª”æ¡ˆ
        srt_path = output_dir / f"{video_id}.srt"
        srt_content = generate_srt_from_segments(result.get('segments', []))
        if srt_content:
            srt_path.write_text(srt_content, encoding="utf-8")
            print(f"ğŸ“º SRT å­—å¹•å·²å„²å­˜è‡³: {srt_path}")

        # æ¸…ç†éŸ³æª”
        _cleanup_audio_files(audio_path)

        return str(transcript_path)

    except Exception as error:
        print(f"å¿«é€Ÿæ¨¡å¼è½‰æ›éŒ¯èª¤: {error}")
        # å¦‚æœå¿«é€Ÿæ¨¡å¼å¤±æ•—ï¼Œå›é€€åˆ°æ™®é€šæ¨¡å¼
        return transcript_to_text(audio_path, video_id)


def generate_srt_from_segments(segments: list) -> str:
    """
    å¾ Whisper çš„ segments ç”Ÿæˆ SRT å­—å¹•å…§å®¹

    Args:
        segments: Whisper è¿”å›çš„ segments åˆ—è¡¨

    Returns:
        SRT æ ¼å¼çš„å­—å¹•å…§å®¹
    """
    if not segments:
        return ""

    srt_lines = []

    for i, segment in enumerate(segments, 1):
        start_time = format_srt_time(segment['start'])
        end_time = format_srt_time(segment['end'])
        text = segment['text'].strip()

        srt_lines.append(f"{i}")
        srt_lines.append(f"{start_time} --> {end_time}")
        srt_lines.append(text)
        srt_lines.append("")  # ç©ºè¡Œåˆ†éš”

    return "\n".join(srt_lines)


def format_srt_time(seconds: float) -> str:
    """
    æ ¼å¼åŒ–æ™‚é–“ç‚º SRT æ ¼å¼ (HH:MM:SS,mmm)

    Args:
        seconds: ç§’æ•¸

    Returns:
        SRT æ™‚é–“æ ¼å¼å­—ä¸²
    """
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    milliseconds = int((seconds % 1) * 1000)

    return f"{hours:02d}:{minutes:02d}:{secs:02d},{milliseconds:03d}"


def _cleanup_audio_files(audio_path: str):
    """
    æ¸…ç†éŸ³æª”æª”æ¡ˆ

    Args:
        audio_path: éŸ³æª”è·¯å¾‘
    """
    audio_extensions = ['.mp3', '.wav', '.m4a', '.aac', '.flac', '.ogg']
    audio_dir = os.path.dirname(audio_path)
    audio_basename = os.path.splitext(os.path.basename(audio_path))[0]

    for ext in audio_extensions:
        audio_file = os.path.join(audio_dir, audio_basename + ext)
        if os.path.exists(audio_file):
            try:
                os.remove(audio_file)
                print(f"ğŸ—‘ï¸ å·²åˆªé™¤éŸ³æª”: {audio_file}")
            except Exception as e:
                print(f"âš ï¸ ç„¡æ³•åˆªé™¤éŸ³æª” {audio_file}: {e}")

    # åˆªé™¤åŸå§‹éŸ³æª”
    if os.path.exists(audio_path):
        try:
            os.remove(audio_path)
            print(f"ğŸ—‘ï¸ å·²åˆªé™¤åŸå§‹éŸ³æª”: {audio_path}")
        except Exception as e:
            print(f"âš ï¸ ç„¡æ³•åˆªé™¤åŸå§‹éŸ³æª”: {e}")


if __name__ == "__main__":
    # æ¸¬è©¦ç”¨
    print("æ­¤æ¨¡çµ„æä¾› Whisper èªéŸ³è­˜åˆ¥åŠŸèƒ½")
    print("è«‹ä½¿ç”¨ main.py åŸ·è¡Œå®Œæ•´çš„å½±ç‰‡è½‰å­—å¹•æµç¨‹")
